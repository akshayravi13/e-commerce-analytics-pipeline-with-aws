{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Event Analytics Pipeline\n",
    "## Build Pipeline Orchestration Notebook\n",
    "\n",
    "**Author:** Akshay Ravi (aravi7)  \n",
    "**Purpose:** This notebook orchestrates the ETL pipeline for transforming raw e-commerce events from JSON to Parquet format.\n",
    "\n",
    "### Pipeline Overview\n",
    "```\n",
    "Raw Events (JSON, gzipped)     →     Glue Spark ETL     →     Processed Events (Parquet)\n",
    "s3://capstone-events-...            (with bookmarks)         s3://capstone-events-processed-...\n",
    "```\n",
    "\n",
    "### What This Notebook Does\n",
    "1. **Configuration** - Set up AWS clients and parameters\n",
    "2. **Upload ETL Script** - Deploy the PySpark transformation code to S3\n",
    "3. **Discover Partitions** - Run MSCK REPAIR on raw events table\n",
    "4. **Execute ETL Job** - Trigger Glue job with bookmark-enabled incremental processing\n",
    "5. **Discover Output Partitions** - Run MSCK REPAIR on processed events table\n",
    "6. **Validate** - Run test queries to verify the pipeline\n",
    "\n",
    "### Idempotency\n",
    "This notebook is designed to be run multiple times safely:\n",
    "- ETL script upload overwrites existing script\n",
    "- Glue job bookmarks ensure only new data is processed\n",
    "- MSCK REPAIR only adds new partitions, doesn't duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Student ID:       aravi7\n",
      "  AWS Account:      410367694421\n",
      "  Region:           us-west-2\n",
      "  Source Bucket:    capstone-events-aravi7-410367694421\n",
      "  Processed Bucket: capstone-events-processed-aravi7-410367694421\n",
      "  Glue Database:    capstone_aravi7_db\n",
      "  Glue Job:         capstone-etl-aravi7\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Update these values for your environment\n",
    "# =============================================================================\n",
    "STUDENT_ID = 'aravi7'\n",
    "AWS_ACCOUNT_ID = '410367694421'\n",
    "AWS_REGION = 'us-west-2'\n",
    "\n",
    "# Derived names (match CloudFormation template)\n",
    "SOURCE_BUCKET = f'capstone-events-{STUDENT_ID}-{AWS_ACCOUNT_ID}'\n",
    "PROCESSED_BUCKET = f'capstone-events-processed-{STUDENT_ID}-{AWS_ACCOUNT_ID}'\n",
    "GLUE_DATABASE = f'capstone_{STUDENT_ID}_db'\n",
    "RAW_TABLE = 'raw_events'\n",
    "PROCESSED_TABLE = 'processed_events'\n",
    "GLUE_JOB_NAME = f'capstone-etl-{STUDENT_ID}'\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "glue_client = boto3.client('glue', region_name=AWS_REGION)\n",
    "athena_client = boto3.client('athena', region_name=AWS_REGION)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Student ID:       {STUDENT_ID}\")\n",
    "print(f\"  AWS Account:      {AWS_ACCOUNT_ID}\")\n",
    "print(f\"  Region:           {AWS_REGION}\")\n",
    "print(f\"  Source Bucket:    {SOURCE_BUCKET}\")\n",
    "print(f\"  Processed Bucket: {PROCESSED_BUCKET}\")\n",
    "print(f\"  Glue Database:    {GLUE_DATABASE}\")\n",
    "print(f\"  Glue Job:         {GLUE_JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Verify Infrastructure Exists\n",
    "\n",
    "Before proceeding, let's verify that the CloudFormation stack has created all required resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying infrastructure...\n",
      "\n",
      "  ✅ Source Bucket\n",
      "  ✅ Processed Bucket\n",
      "  ✅ Glue Database\n",
      "  ✅ Raw Events Table\n",
      "  ✅ Processed Events Table\n",
      "  ✅ Glue ETL Job\n",
      "\n",
      "✅ All infrastructure components verified!\n"
     ]
    }
   ],
   "source": [
    "def check_bucket_exists(bucket_name):\n",
    "    \"\"\"Check if S3 bucket exists.\"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_glue_database_exists(database_name):\n",
    "    \"\"\"Check if Glue database exists.\"\"\"\n",
    "    try:\n",
    "        glue_client.get_database(Name=database_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_glue_table_exists(database_name, table_name):\n",
    "    \"\"\"Check if Glue table exists.\"\"\"\n",
    "    try:\n",
    "        glue_client.get_table(DatabaseName=database_name, Name=table_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_glue_job_exists(job_name):\n",
    "    \"\"\"Check if Glue job exists.\"\"\"\n",
    "    try:\n",
    "        glue_client.get_job(JobName=job_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Verify all resources\n",
    "print(\"Verifying infrastructure...\\n\")\n",
    "\n",
    "checks = [\n",
    "    (\"Source Bucket\", check_bucket_exists(SOURCE_BUCKET)),\n",
    "    (\"Processed Bucket\", check_bucket_exists(PROCESSED_BUCKET)),\n",
    "    (\"Glue Database\", check_glue_database_exists(GLUE_DATABASE)),\n",
    "    (\"Raw Events Table\", check_glue_table_exists(GLUE_DATABASE, RAW_TABLE)),\n",
    "    (\"Processed Events Table\", check_glue_table_exists(GLUE_DATABASE, PROCESSED_TABLE)),\n",
    "    (\"Glue ETL Job\", check_glue_job_exists(GLUE_JOB_NAME)),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for name, exists in checks:\n",
    "    status = \"✅\" if exists else \"❌\"\n",
    "    print(f\"  {status} {name}\")\n",
    "    if not exists:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n✅ All infrastructure components verified!\")\n",
    "else:\n",
    "    print(\"\\n❌ Some components missing. Please check your CloudFormation stack.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Check Raw Data Availability\n",
    "\n",
    "Let's see how much raw data has been generated by the Lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Summary:\n",
      "  Total files:     152\n",
      "  Total size:      1537.28 MB\n",
      "  Est. events:     95,000,000 (avg 625K per file)\n",
      "  Est. data hours: 12.7 hours (12 files per hour)\n",
      "\n",
      "Most recent files:\n",
      "  events/year=2025/month=12/day=06/hour=21/minute=21/events-20251206-212118.jsonl.gz (9.27 MB)\n",
      "  events/year=2025/month=12/day=06/hour=21/minute=16/events-20251206-211623.jsonl.gz (11.69 MB)\n",
      "  events/year=2025/month=12/day=06/hour=21/minute=11/events-20251206-211122.jsonl.gz (11.46 MB)\n",
      "  events/year=2025/month=12/day=06/hour=21/minute=06/events-20251206-210618.jsonl.gz (8.99 MB)\n",
      "  events/year=2025/month=12/day=06/hour=21/minute=01/events-20251206-210116.jsonl.gz (8.23 MB)\n"
     ]
    }
   ],
   "source": [
    "def count_raw_files():\n",
    "    \"\"\"Count the number of raw event files in the source bucket.\"\"\"\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    \n",
    "    file_count = 0\n",
    "    total_size = 0\n",
    "    files = []\n",
    "    \n",
    "    for page in paginator.paginate(Bucket=SOURCE_BUCKET, Prefix='events/'):\n",
    "        for obj in page.get('Contents', []):\n",
    "            if obj['Key'].endswith('.jsonl.gz'):\n",
    "                file_count += 1\n",
    "                total_size += obj['Size']\n",
    "                files.append({\n",
    "                    'key': obj['Key'],\n",
    "                    'size_mb': round(obj['Size'] / (1024*1024), 2),\n",
    "                    'last_modified': obj['LastModified']\n",
    "                })\n",
    "    \n",
    "    return file_count, total_size, files\n",
    "\n",
    "file_count, total_size, files = count_raw_files()\n",
    "\n",
    "print(f\"Raw Data Summary:\")\n",
    "print(f\"  Total files:     {file_count}\")\n",
    "print(f\"  Total size:      {round(total_size / (1024*1024), 2)} MB\")\n",
    "print(f\"  Est. events:     {file_count * 625000:,} (avg 625K per file)\")\n",
    "print(f\"  Est. data hours: {file_count / 12:.1f} hours (12 files per hour)\")\n",
    "\n",
    "\n",
    "# Show recent files\n",
    "print(f\"\\nMost recent files:\")\n",
    "for f in sorted(files, key=lambda x: x['last_modified'], reverse=True)[:5]:\n",
    "    print(f\"  {f['key']} ({f['size_mb']} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Upload ETL Script to S3\n",
    "\n",
    "The Glue job needs its PySpark script stored in S3. This script:\n",
    "- Reads raw JSON events using Glue bookmarks (incremental processing)\n",
    "- Parses timestamps and adds derived columns\n",
    "- Writes output as partitioned Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ETL script uploaded to s3://capstone-events-processed-aravi7-410367694421/scripts/etl_job.py\n"
     ]
    }
   ],
   "source": [
    " # ETL Script - PySpark code for transforming JSON to Parquet\n",
    "\n",
    "ETL_SCRIPT = '''\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, hour, year, month, dayofmonth, lpad\n",
    "\n",
    "# Initialize Glue context\n",
    "args = getResolvedOptions(sys.argv, [\n",
    "    'JOB_NAME',\n",
    "    'SOURCE_DATABASE',\n",
    "    'SOURCE_TABLE',\n",
    "    'TARGET_PATH',\n",
    "    'TARGET_DATABASE',\n",
    "    'TARGET_TABLE'\n",
    "])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "print(f\"Starting ETL job: {args['JOB_NAME']}\")\n",
    "print(f\"Source: {args['SOURCE_DATABASE']}.{args['SOURCE_TABLE']}\")\n",
    "print(f\"Target: {args['TARGET_PATH']}\")\n",
    "\n",
    "# Read from source table with bookmarks enabled\n",
    "print(\"Reading source data with bookmarks...\")\n",
    "\n",
    "source_dyf = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=args['SOURCE_DATABASE'],\n",
    "    table_name=args['SOURCE_TABLE'],\n",
    "    transformation_ctx=\"source_data\"\n",
    ")\n",
    "\n",
    "record_count = source_dyf.count()\n",
    "print(f\"Records to process: {record_count:,}\")\n",
    "\n",
    "if record_count == 0:\n",
    "    print(\"No new data to process. Exiting.\")\n",
    "    job.commit()\n",
    "    sys.exit(0)\n",
    "\n",
    "# Transform\n",
    "print(\"Applying transformations...\")\n",
    "\n",
    "df = source_dyf.toDF()\n",
    "\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"event_date\", to_date(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"event_hour\", hour(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"year\", year(col(\"timestamp\")).cast(\"string\")) \\\n",
    "    .withColumn(\"month\", lpad(month(col(\"timestamp\")).cast(\"string\"), 2, \"0\")) \\\n",
    "    .withColumn(\"day\", lpad(dayofmonth(col(\"timestamp\")).cast(\"string\"), 2, \"0\")) \\\n",
    "    .withColumn(\"hour\", lpad(hour(col(\"timestamp\")).cast(\"string\"), 2, \"0\"))\n",
    "\n",
    "df_final = df_transformed.select(\n",
    "    \"timestamp\", \"user_id\", \"session_id\", \"event_type\", \"product_id\",\n",
    "    \"quantity\", \"price\", \"category\", \"search_query\", \"event_date\", \"event_hour\",\n",
    "    \"year\", \"month\", \"day\", \"hour\"\n",
    ")\n",
    "\n",
    "print(\"Schema after transformation:\")\n",
    "df_final.printSchema()\n",
    "\n",
    "# Write to Parquet\n",
    "print(\"Writing to Parquet...\")\n",
    "\n",
    "output_dyf = DynamicFrame.fromDF(df_final, glueContext, \"output_data\")\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=output_dyf,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\n",
    "        \"path\": args['TARGET_PATH'],\n",
    "        \"partitionKeys\": [\"year\", \"month\", \"day\", \"hour\"]\n",
    "    },\n",
    "    format=\"parquet\",\n",
    "    format_options={\"compression\": \"snappy\"},\n",
    "    transformation_ctx=\"sink_data\"\n",
    ")\n",
    "\n",
    "print(f\"Successfully wrote {record_count:,} records to {args['TARGET_PATH']}\")\n",
    "job.commit()\n",
    "print(\"Job completed successfully!\")\n",
    "'''\n",
    "\n",
    "\n",
    "# Upload script to S3\n",
    "script_key = 'scripts/etl_job.py'\n",
    "\n",
    "s3_client.put_object(\n",
    "    Bucket=PROCESSED_BUCKET,\n",
    "    Key=script_key,\n",
    "    Body=ETL_SCRIPT.encode('utf-8'),\n",
    "    ContentType='text/x-python'\n",
    ")\n",
    "\n",
    "print(f\"✅ ETL script uploaded to s3://{PROCESSED_BUCKET}/{script_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Discover Raw Data Partitions (MSCK REPAIR)\n",
    "\n",
    "Before running the ETL job, we need to tell Glue about the partitions in our raw data. The Lambda function writes data with Hive-style partitioning, but the Glue catalog doesn't automatically discover new partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering partitions in raw_events table...\n",
      "Running: MSCK REPAIR TABLE capstone_aravi7_db.raw_events\n",
      "\n",
      "Query started: 5a8230dd-1e17-4eba-a089-2a3356480823\n",
      "  Status: QUEUED...\n",
      "  Status: RUNNING...\n",
      "  Status: RUNNING...\n",
      "  ✅ Query completed successfully\n"
     ]
    }
   ],
   "source": [
    "def run_athena_query(query, database, wait=True):\n",
    "    \"\"\"\n",
    "    Execute an Athena query and optionally wait for completion.\n",
    "    Returns query execution ID and results if wait=True.\n",
    "    \"\"\"\n",
    "    # Start query execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': database},\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': f's3://{PROCESSED_BUCKET}/athena-results/'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    execution_id = response['QueryExecutionId']\n",
    "    print(f\"Query started: {execution_id}\")\n",
    "    \n",
    "    if not wait:\n",
    "        return execution_id, None\n",
    "    \n",
    "    # Wait for query to complete\n",
    "    while True:\n",
    "        status_response = athena_client.get_query_execution(\n",
    "            QueryExecutionId=execution_id\n",
    "        )\n",
    "        status = status_response['QueryExecution']['Status']['State']\n",
    "        \n",
    "        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            break\n",
    "        \n",
    "        print(f\"  Status: {status}...\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    if status == 'SUCCEEDED':\n",
    "        print(f\"  ✅ Query completed successfully\")\n",
    "        \n",
    "        # Get results\n",
    "        results = athena_client.get_query_results(\n",
    "            QueryExecutionId=execution_id\n",
    "        )\n",
    "        return execution_id, results\n",
    "    else:\n",
    "        error_message = status_response['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')\n",
    "        print(f\"  ❌ Query failed: {error_message}\")\n",
    "        return execution_id, None\n",
    "\n",
    "\n",
    "# Run MSCK REPAIR on raw_events table\n",
    "print(\"Discovering partitions in raw_events table...\")\n",
    "print(f\"Running: MSCK REPAIR TABLE {GLUE_DATABASE}.{RAW_TABLE}\\n\")\n",
    "\n",
    "execution_id, results = run_athena_query(\n",
    "    f\"MSCK REPAIR TABLE {RAW_TABLE}\",\n",
    "    GLUE_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying partitions in raw_events table...\n",
      "Query started: 408eeea4-d760-41a5-b09c-bcfee9850068\n",
      "  Status: QUEUED...\n",
      "  Status: RUNNING...\n",
      "  ✅ Query completed successfully\n",
      "\n",
      "Found 14 distinct hour partitions:\n",
      "  year=2025/month=12/day=06/hour=08\n",
      "  year=2025/month=12/day=06/hour=09\n",
      "  year=2025/month=12/day=06/hour=10\n",
      "  year=2025/month=12/day=06/hour=11\n",
      "  year=2025/month=12/day=06/hour=12\n",
      "  year=2025/month=12/day=06/hour=13\n",
      "  year=2025/month=12/day=06/hour=14\n",
      "  year=2025/month=12/day=06/hour=15\n",
      "  year=2025/month=12/day=06/hour=16\n",
      "  year=2025/month=12/day=06/hour=17\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Verify partitions were discovered\n",
    "print(\"\\nVerifying partitions in raw_events table...\")\n",
    "\n",
    "execution_id, results = run_athena_query(\n",
    "    f\"SELECT DISTINCT year, month, day, hour FROM {RAW_TABLE} ORDER BY year, month, day, hour\",\n",
    "    GLUE_DATABASE\n",
    ")\n",
    "\n",
    "if results:\n",
    "    rows = results['ResultSet']['Rows'][1:]  # Skip header\n",
    "    print(f\"\\nFound {len(rows)} distinct hour partitions:\")\n",
    "    for row in rows[:10]:  # Show first 10\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(f\"  year={values[0]}/month={values[1]}/day={values[2]}/hour={values[3]}\")\n",
    "    if len(rows) > 10:\n",
    "        print(f\"  ... and {len(rows) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Run Glue ETL Job\n",
    "\n",
    "Now we trigger the Glue ETL job. Key features:\n",
    "- **Bookmarks enabled**: Only processes files not seen in previous runs\n",
    "- **Idempotent**: Safe to run multiple times\n",
    "- **Incremental**: New runs only process new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Glue job: capstone-etl-aravi7\n",
      "Job run started: jr_11178f1ec6f01f52c67f0fd90bc10931f5b50b09fc5ecdfce16e48a390a8b859\n",
      "Waiting for job to complete...\n",
      "  Status: RUNNING (elapsed: 0s)\n",
      "  Status: RUNNING (elapsed: 23s)\n",
      "  Status: RUNNING (elapsed: 53s)\n",
      "  Status: RUNNING (elapsed: 83s)\n",
      "  Status: RUNNING (elapsed: 113s)\n",
      "  Status: RUNNING (elapsed: 143s)\n",
      "  Status: RUNNING (elapsed: 174s)\n",
      "  Status: RUNNING (elapsed: 204s)\n",
      "  Status: RUNNING (elapsed: 234s)\n",
      "  Status: RUNNING (elapsed: 264s)\n",
      "  Status: RUNNING (elapsed: 294s)\n",
      "  Status: RUNNING (elapsed: 324s)\n",
      "  Status: RUNNING (elapsed: 354s)\n",
      "  Status: RUNNING (elapsed: 384s)\n",
      "  Status: RUNNING (elapsed: 415s)\n",
      "  Status: RUNNING (elapsed: 445s)\n",
      "  Status: RUNNING (elapsed: 475s)\n",
      "  Status: RUNNING (elapsed: 505s)\n",
      "  Status: RUNNING (elapsed: 535s)\n",
      "  Status: RUNNING (elapsed: 565s)\n",
      "  Status: RUNNING (elapsed: 595s)\n",
      "  Status: RUNNING (elapsed: 625s)\n",
      "  Status: RUNNING (elapsed: 655s)\n",
      "  Status: RUNNING (elapsed: 686s)\n",
      "  Status: RUNNING (elapsed: 716s)\n",
      "  Status: RUNNING (elapsed: 746s)\n",
      "  Status: RUNNING (elapsed: 776s)\n",
      "  Status: RUNNING (elapsed: 806s)\n",
      "  Status: RUNNING (elapsed: 836s)\n",
      "  Status: RUNNING (elapsed: 866s)\n",
      "  Status: RUNNING (elapsed: 896s)\n",
      "  Status: RUNNING (elapsed: 927s)\n",
      "  Status: RUNNING (elapsed: 957s)\n",
      "  Status: RUNNING (elapsed: 987s)\n",
      "  Status: RUNNING (elapsed: 1017s)\n",
      "  Status: RUNNING (elapsed: 1047s)\n",
      "  Status: RUNNING (elapsed: 1077s)\n",
      "  Status: RUNNING (elapsed: 1107s)\n",
      "  Status: RUNNING (elapsed: 1138s)\n",
      "  Status: RUNNING (elapsed: 1168s)\n",
      "  Status: RUNNING (elapsed: 1198s)\n",
      "  Status: RUNNING (elapsed: 1228s)\n",
      "  Status: RUNNING (elapsed: 1258s)\n",
      "  Status: RUNNING (elapsed: 1288s)\n",
      "  Status: RUNNING (elapsed: 1318s)\n",
      "  Status: RUNNING (elapsed: 1348s)\n",
      "  Status: RUNNING (elapsed: 1379s)\n",
      "  Status: RUNNING (elapsed: 1409s)\n",
      "  Status: RUNNING (elapsed: 1439s)\n",
      "  Status: RUNNING (elapsed: 1469s)\n",
      "  Status: RUNNING (elapsed: 1499s)\n",
      "  Status: RUNNING (elapsed: 1529s)\n",
      "  Status: RUNNING (elapsed: 1559s)\n",
      "  Status: RUNNING (elapsed: 1589s)\n",
      "  Status: RUNNING (elapsed: 1619s)\n",
      "  Status: RUNNING (elapsed: 1650s)\n",
      "  Status: RUNNING (elapsed: 1680s)\n",
      "\n",
      "✅ Job completed successfully in 1710 seconds\n"
     ]
    }
   ],
   "source": [
    "def run_glue_job(job_name, wait=True):\n",
    "    \"\"\"\n",
    "    Start a Glue job and optionally wait for completion.\n",
    "    \"\"\"\n",
    "    print(f\"Starting Glue job: {job_name}\")\n",
    "    \n",
    "    # Start job run\n",
    "    response = glue_client.start_job_run(JobName=job_name)\n",
    "    run_id = response['JobRunId']\n",
    "    print(f\"Job run started: {run_id}\")\n",
    "    \n",
    "    if not wait:\n",
    "        return run_id, None\n",
    "    \n",
    "    # Wait for completion\n",
    "    print(\"Waiting for job to complete...\")\n",
    "    \n",
    "    while True:\n",
    "        status_response = glue_client.get_job_run(\n",
    "            JobName=job_name,\n",
    "            RunId=run_id\n",
    "        )\n",
    "        status = status_response['JobRun']['JobRunState']\n",
    "        \n",
    "        if status in ['SUCCEEDED', 'FAILED', 'STOPPED', 'TIMEOUT']:\n",
    "            break\n",
    "        \n",
    "        # Show progress\n",
    "        elapsed = status_response['JobRun'].get('ExecutionTime', 0)\n",
    "        print(f\"  Status: {status} (elapsed: {elapsed}s)\")\n",
    "        time.sleep(30)  # Check every 30 seconds\n",
    "    \n",
    "    # Final status\n",
    "    if status == 'SUCCEEDED':\n",
    "        duration = status_response['JobRun'].get('ExecutionTime', 0)\n",
    "        print(f\"\\n✅ Job completed successfully in {duration} seconds\")\n",
    "        return run_id, status_response['JobRun']\n",
    "    else:\n",
    "        error_message = status_response['JobRun'].get('ErrorMessage', 'Unknown error')\n",
    "        print(f\"\\n❌ Job failed with status: {status}\")\n",
    "        print(f\"   Error: {error_message}\")\n",
    "        return run_id, status_response['JobRun']\n",
    "\n",
    "\n",
    "# Run the ETL job\n",
    "run_id, job_run = run_glue_job(GLUE_JOB_NAME, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ⚠️ Note on Incremental Processing\n",
    "\n",
    "**This notebook demonstrates incremental ETL processing.**\n",
    "\n",
    "The pipeline was initially run on **15 files (~8.5 million events, 1.2 hours of data)** to validate the setup. That first run completed in **254 seconds**.\n",
    "\n",
    "This current execution is running **~14 hours later**, with **152 total files** now in the source bucket. Thanks to Glue job bookmarks, the ETL job will **only process the new 137 files** (~86 million new events), skipping the 15 files that were already processed in the first run.\n",
    "\n",
    "This is the key value of bookmark-enabled incremental processing:\n",
    "- ✅ No reprocessing of historical data\n",
    "- ✅ Costs scale with new data only\n",
    "- ✅ Idempotent - safe to run multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Discover Processed Data Partitions\n",
    "\n",
    "After the ETL job completes, we need to update the Glue catalog with the new Parquet partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking processed data output...\n",
      "\n",
      "Processed Data Summary:\n",
      "  Parquet files: 152\n",
      "  Total size:    1875.24 MB\n",
      "\n",
      "Sample files:\n",
      "  events/year=2025/month=12/day=06/hour=08/part-00002-7305b1b3-ea16-4c0b-b116-81b0fd84c035.c000.snappy.parquet (14.35 MB)\n",
      "  events/year=2025/month=12/day=06/hour=08/part-00005-7305b1b3-ea16-4c0b-b116-81b0fd84c035.c000.snappy.parquet (12.5 MB)\n",
      "  events/year=2025/month=12/day=06/hour=08/part-00012-7305b1b3-ea16-4c0b-b116-81b0fd84c035.c000.snappy.parquet (12.17 MB)\n",
      "  events/year=2025/month=12/day=06/hour=09/part-00000-7305b1b3-ea16-4c0b-b116-81b0fd84c035.c000.snappy.parquet (10.25 MB)\n",
      "  events/year=2025/month=12/day=06/hour=09/part-00001-7305b1b3-ea16-4c0b-b116-81b0fd84c035.c000.snappy.parquet (10.03 MB)\n"
     ]
    }
   ],
   "source": [
    "# Check what was written to the processed bucket\n",
    "print(\"Checking processed data output...\\n\")\n",
    "\n",
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "parquet_files = []\n",
    "\n",
    "for page in paginator.paginate(Bucket=PROCESSED_BUCKET, Prefix='events/'):\n",
    "    for obj in page.get('Contents', []):\n",
    "        if obj['Key'].endswith('.parquet'):\n",
    "            parquet_files.append({\n",
    "                'key': obj['Key'],\n",
    "                'size_mb': round(obj['Size'] / (1024*1024), 2)\n",
    "            })\n",
    "\n",
    "total_size = sum(f['size_mb'] for f in parquet_files)\n",
    "\n",
    "print(f\"Processed Data Summary:\")\n",
    "print(f\"  Parquet files: {len(parquet_files)}\")\n",
    "print(f\"  Total size:    {total_size:.2f} MB\")\n",
    "\n",
    "if parquet_files:\n",
    "    print(f\"\\nSample files:\")\n",
    "    for f in parquet_files[:5]:\n",
    "        print(f\"  {f['key']} ({f['size_mb']} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering partitions in processed_events table...\n",
      "Running: MSCK REPAIR TABLE capstone_aravi7_db.processed_events\n",
      "\n",
      "Query started: b80d716d-5d85-407f-9480-fe27a0a5d356\n",
      "  Status: QUEUED...\n",
      "  Status: RUNNING...\n",
      "  ✅ Query completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Run MSCK REPAIR on processed_events table\n",
    "print(\"Discovering partitions in processed_events table...\")\n",
    "print(f\"Running: MSCK REPAIR TABLE {GLUE_DATABASE}.{PROCESSED_TABLE}\\n\")\n",
    "\n",
    "execution_id, results = run_athena_query(\n",
    "    f\"MSCK REPAIR TABLE {PROCESSED_TABLE}\",\n",
    "    GLUE_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Validate Pipeline Output\n",
    "\n",
    "Let's run some validation queries to ensure the pipeline worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating processed data...\n",
      "\n",
      "Query started: 9aaf2fba-60ac-44e2-a3ad-248c4571e1a5\n",
      "  Status: QUEUED...\n",
      "  ✅ Query completed successfully\n",
      "Total records in processed_events: 94,383,522\n"
     ]
    }
   ],
   "source": [
    "# Count records in processed table\n",
    "print(\"Validating processed data...\\n\")\n",
    "\n",
    "execution_id, results = run_athena_query(\n",
    "    f\"SELECT COUNT(*) as total_records FROM {PROCESSED_TABLE}\",\n",
    "    GLUE_DATABASE\n",
    ")\n",
    "\n",
    "if results:\n",
    "    count = results['ResultSet']['Rows'][1]['Data'][0]['VarCharValue']\n",
    "    print(f\"Total records in processed_events: {int(count):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event type distribution:\n",
      "\n",
      "Query started: 760d4cb3-31e1-4c92-b5f2-49e79a71c688\n",
      "  Status: QUEUED...\n",
      "  ✅ Query completed successfully\n",
      "Event Type                     Count   Percentage\n",
      "--------------------------------------------------\n",
      "page_view                 47,184,334       49.99%\n",
      "add_to_cart               18,882,867       20.01%\n",
      "remove_from_cart           9,444,106       10.01%\n",
      "purchase                   9,439,073        10.0%\n",
      "search                     9,433,142        9.99%\n"
     ]
    }
   ],
   "source": [
    "# Check event type distribution\n",
    "print(\"Event type distribution:\\n\")\n",
    "\n",
    "execution_id, results = run_athena_query(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        event_type, \n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM {PROCESSED_TABLE}\n",
    "    GROUP BY event_type\n",
    "    ORDER BY count DESC\n",
    "    \"\"\",\n",
    "    GLUE_DATABASE\n",
    ")\n",
    "\n",
    "if results:\n",
    "    print(f\"{'Event Type':<20} {'Count':>15} {'Percentage':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for row in results['ResultSet']['Rows'][1:]:\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(f\"{values[0]:<20} {int(values[1]):>15,} {values[2]:>11}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data partitions:\n",
      "\n",
      "Query started: 437ee074-936f-4f1e-b82c-31a41e0615c5\n",
      "  Status: QUEUED...\n",
      "  ✅ Query completed successfully\n",
      "Partition                                   Records\n",
      "----------------------------------------------------\n",
      "year=2025/month=12/day=06/hour=08         1,965,478\n",
      "year=2025/month=12/day=06/hour=09         7,127,272\n",
      "year=2025/month=12/day=06/hour=10         7,375,367\n",
      "year=2025/month=12/day=06/hour=11         7,877,870\n",
      "year=2025/month=12/day=06/hour=12         7,720,866\n",
      "year=2025/month=12/day=06/hour=13         7,390,843\n",
      "year=2025/month=12/day=06/hour=14         7,453,672\n",
      "year=2025/month=12/day=06/hour=15         7,699,679\n",
      "year=2025/month=12/day=06/hour=16         7,370,158\n",
      "year=2025/month=12/day=06/hour=17         7,208,048\n",
      "year=2025/month=12/day=06/hour=18         7,476,351\n",
      "year=2025/month=12/day=06/hour=19         7,240,978\n",
      "year=2025/month=12/day=06/hour=20         7,427,761\n",
      "year=2025/month=12/day=06/hour=21         3,049,179\n"
     ]
    }
   ],
   "source": [
    "# Check partitions in processed data\n",
    "print(\"Processed data partitions:\\n\")\n",
    "\n",
    "execution_id, results = run_athena_query(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        year, month, day, hour,\n",
    "        COUNT(*) as record_count\n",
    "    FROM {PROCESSED_TABLE}\n",
    "    GROUP BY year, month, day, hour\n",
    "    ORDER BY year, month, day, hour\n",
    "    \"\"\",\n",
    "    GLUE_DATABASE\n",
    ")\n",
    "\n",
    "if results:\n",
    "    print(f\"{'Partition':<35} {'Records':>15}\")\n",
    "    print(\"-\" * 52)\n",
    "    for row in results['ResultSet']['Rows'][1:]:\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        partition = f\"year={values[0]}/month={values[1]}/day={values[2]}/hour={values[3]}\"\n",
    "        print(f\"{partition:<35} {int(values[4]):>15,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Run the 5 Required Queries\n",
    "\n",
    "Finally, let's run all 5 required queries to validate our analytical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY 1: Conversion Funnel\n",
      "======================================================================\n",
      "Query started: 6040ef23-1ae9-45ab-a188-eb4db0595d85\n",
      "  Status: QUEUED...\n",
      "  ✅ Query completed successfully\n",
      "\n",
      "Product           Views      Carts  Purchases      V→C %      C→P %\n",
      "-----------------------------------------------------------------\n",
      "p_3001        1,575,319    629,563    313,829     39.96%     49.85%\n",
      "p_1005        1,574,487    627,893    314,460     39.88%     50.08%\n",
      "p_1001        1,574,465    629,406    315,007     39.98%     50.05%\n",
      "p_5003        1,574,392    629,643    314,771     39.99%     49.99%\n",
      "p_3004        1,574,301    630,454    313,983     40.05%      49.8%\n",
      "p_1003        1,573,627    630,137    314,988     40.04%     49.99%\n",
      "p_5004        1,573,607    629,010    314,122     39.97%     49.94%\n",
      "p_1002        1,573,515    628,239    314,772     39.93%      50.1%\n",
      "p_5002        1,573,440    628,542    314,407     39.95%     50.02%\n",
      "p_6005        1,573,205    630,315    314,374     40.07%     49.88%\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Conversion Funnel\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY 1: Conversion Funnel\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query1 = f\"\"\"\n",
    "SELECT \n",
    "    product_id,\n",
    "    COUNT(CASE WHEN event_type = 'page_view' THEN 1 END) AS view_count,\n",
    "    COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END) AS add_to_cart_count,\n",
    "    COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchase_count,\n",
    "    ROUND(\n",
    "        CAST(COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END) AS DOUBLE) / \n",
    "        NULLIF(COUNT(CASE WHEN event_type = 'page_view' THEN 1 END), 0) * 100, \n",
    "        2\n",
    "    ) AS view_to_cart_rate_pct,\n",
    "    ROUND(\n",
    "        CAST(COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS DOUBLE) / \n",
    "        NULLIF(COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END), 0) * 100, \n",
    "        2\n",
    "    ) AS cart_to_purchase_rate_pct\n",
    "FROM {PROCESSED_TABLE}\n",
    "WHERE product_id IS NOT NULL\n",
    "GROUP BY product_id\n",
    "ORDER BY view_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "execution_id, results = run_athena_query(query1, GLUE_DATABASE)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'Product':<12} {'Views':>10} {'Carts':>10} {'Purchases':>10} {'V→C %':>10} {'C→P %':>10}\")\n",
    "    print(\"-\" * 65)\n",
    "    for row in results['ResultSet']['Rows'][1:]:\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(f\"{values[0]:<12} {int(values[1]):>10,} {int(values[2]):>10,} {int(values[3]):>10,} {values[4]:>9}% {values[5]:>9}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY 2: Hourly Revenue\n",
      "======================================================================\n",
      "Query started: 6d7a14b6-0b71-4730-a7a1-3614f7597793\n",
      "  Status: QUEUED...\n",
      "  ✅ Query completed successfully\n",
      "\n",
      "Hour                              Revenue    Purchases    Avg Order\n",
      "-----------------------------------------------------------------\n",
      "2025-12-06 08:00:00.000   $ 91,526,394.67      196,718 $     465.27\n",
      "2025-12-06 09:00:00.000   $331,330,239.79      712,862 $     464.79\n",
      "2025-12-06 10:00:00.000   $343,023,225.10      737,960 $     464.83\n",
      "2025-12-06 11:00:00.000   $366,600,468.53      788,002 $     465.23\n",
      "2025-12-06 12:00:00.000   $358,347,867.81      771,372 $     464.56\n",
      "2025-12-06 13:00:00.000   $343,224,052.85      738,759 $     464.60\n",
      "2025-12-06 14:00:00.000   $346,099,943.73      744,343 $     464.97\n",
      "2025-12-06 15:00:00.000   $358,354,212.65      770,715 $     464.96\n",
      "2025-12-06 16:00:00.000   $343,080,118.47      737,882 $     464.95\n",
      "2025-12-06 17:00:00.000   $335,286,832.18      720,866 $     465.12\n",
      "2025-12-06 18:00:00.000   $347,664,570.00      747,385 $     465.17\n",
      "2025-12-06 19:00:00.000   $337,433,896.76      724,789 $     465.56\n",
      "2025-12-06 20:00:00.000   $345,548,483.12      743,259 $     464.91\n",
      "2025-12-06 21:00:00.000   $141,862,846.57      304,161 $     466.41\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Hourly Revenue\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY 2: Hourly Revenue\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query2 = f\"\"\"\n",
    "SELECT \n",
    "    DATE_TRUNC('hour', timestamp) AS revenue_hour,\n",
    "    ROUND(SUM(price * quantity), 2) AS total_revenue,\n",
    "    COUNT(*) AS purchase_count,\n",
    "    ROUND(AVG(price * quantity), 2) AS avg_order_value\n",
    "FROM {PROCESSED_TABLE}\n",
    "WHERE event_type = 'purchase'\n",
    "GROUP BY DATE_TRUNC('hour', timestamp)\n",
    "ORDER BY revenue_hour\n",
    "\"\"\"\n",
    "\n",
    "execution_id, results = run_athena_query(query2, GLUE_DATABASE)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'Hour':<25} {'Revenue':>15} {'Purchases':>12} {'Avg Order':>12}\")\n",
    "    print(\"-\" * 65)\n",
    "    for row in results['ResultSet']['Rows'][1:]:\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(f\"{values[0]:<25} ${float(values[1]):>14,.2f} {int(values[2]):>12,} ${float(values[3]):>11,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY 3: Top 10 Products by Views\n",
      "======================================================================\n",
      "Query started: fc058e37-e494-46b6-a65b-0f8694cf38e4\n",
      "  Status: QUEUED...\n",
      "  ✅ Query completed successfully\n",
      "\n",
      "Rank   Product      Category               Views\n",
      "--------------------------------------------------\n",
      "1      p_3001       home               1,575,319\n",
      "2      p_1005       electronics        1,574,487\n",
      "3      p_1001       electronics        1,574,465\n",
      "4      p_5003       sports             1,574,392\n",
      "5      p_3004       home               1,574,301\n",
      "6      p_1003       electronics        1,573,627\n",
      "7      p_5004       sports             1,573,607\n",
      "8      p_1002       electronics        1,573,515\n",
      "9      p_5002       sports             1,573,440\n",
      "10     p_6005       toys               1,573,205\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Top 10 Products by Views\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY 3: Top 10 Products by Views\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query3 = f\"\"\"\n",
    "SELECT \n",
    "    product_id,\n",
    "    category,\n",
    "    COUNT(*) AS view_count\n",
    "FROM {PROCESSED_TABLE}\n",
    "WHERE event_type = 'page_view'\n",
    "GROUP BY product_id, category\n",
    "ORDER BY view_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "execution_id, results = run_athena_query(query3, GLUE_DATABASE)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'Rank':<6} {'Product':<12} {'Category':<15} {'Views':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, row in enumerate(results['ResultSet']['Rows'][1:], 1):\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(f\"{i:<6} {values[0]:<12} {values[1]:<15} {int(values[2]):>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY 4: Category Performance (Daily)\n",
      "======================================================================\n",
      "Query started: 54e36e12-2807-4ab9-a7de-69db0641bea8\n",
      "  Status: QUEUED...\n",
      "  ✅ Query completed successfully\n",
      "\n",
      "Category        Date              Total      Views      Carts  Purchases\n",
      "----------------------------------------------------------------------\n",
      "books           2025-12-06   14,150,338  7,859,415  3,143,641  1,574,437\n",
      "clothing        2025-12-06   14,157,363  7,861,952  3,148,340  1,574,409\n",
      "electronics     2025-12-06   14,163,313  7,867,771  3,146,438  1,573,472\n",
      "home            2025-12-06   14,165,166  7,868,531  3,150,940  1,571,755\n",
      "sports          2025-12-06   14,158,856  7,864,183  3,147,361  1,572,759\n",
      "toys            2025-12-06   14,155,344  7,862,482  3,146,147  1,572,241\n"
     ]
    }
   ],
   "source": [
    "# Query 4: Category Performance\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY 4: Category Performance (Daily)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query4 = f\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    event_date,\n",
    "    COUNT(*) AS total_events,\n",
    "    COUNT(CASE WHEN event_type = 'page_view' THEN 1 END) AS page_views,\n",
    "    COUNT(CASE WHEN event_type = 'add_to_cart' THEN 1 END) AS add_to_carts,\n",
    "    COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS purchases\n",
    "FROM {PROCESSED_TABLE}\n",
    "WHERE category IS NOT NULL\n",
    "GROUP BY category, event_date\n",
    "ORDER BY event_date, category\n",
    "\"\"\"\n",
    "\n",
    "execution_id, results = run_athena_query(query4, GLUE_DATABASE)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'Category':<15} {'Date':<12} {'Total':>10} {'Views':>10} {'Carts':>10} {'Purchases':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    for row in results['ResultSet']['Rows'][1:15]:  # Show first 15 rows\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(f\"{values[0]:<15} {values[1]:<12} {int(values[2]):>10,} {int(values[3]):>10,} {int(values[4]):>10,} {int(values[5]):>10,}\")\n",
    "    \n",
    "    total_rows = len(results['ResultSet']['Rows']) - 1\n",
    "    if total_rows > 15:\n",
    "        print(f\"... and {total_rows - 15} more rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY 5: User Activity (Daily)\n",
      "======================================================================\n",
      "Query started: 55562d2d-b894-489e-bd2c-462b756d3600\n",
      "  Status: QUEUED...\n",
      "  Status: RUNNING...\n",
      "  ✅ Query completed successfully\n",
      "\n",
      "Date                Users     Sessions          Events  Events/Session\n",
      "----------------------------------------------------------------------\n",
      "2025-12-06         90,000       90,000      94,383,522         1048.71\n"
     ]
    }
   ],
   "source": [
    "# Query 5: User Activity\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY 5: User Activity (Daily)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query5 = f\"\"\"\n",
    "SELECT \n",
    "    event_date,\n",
    "    COUNT(DISTINCT user_id) AS unique_users,\n",
    "    COUNT(DISTINCT session_id) AS unique_sessions,\n",
    "    COUNT(*) AS total_events,\n",
    "    ROUND(CAST(COUNT(*) AS DOUBLE) / COUNT(DISTINCT session_id), 2) AS events_per_session\n",
    "FROM {PROCESSED_TABLE}\n",
    "GROUP BY event_date\n",
    "ORDER BY event_date\n",
    "\"\"\"\n",
    "\n",
    "execution_id, results = run_athena_query(query5, GLUE_DATABASE)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'Date':<12} {'Users':>12} {'Sessions':>12} {'Events':>15} {'Events/Session':>15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for row in results['ResultSet']['Rows'][1:]:\n",
    "        values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(f\"{values[0]:<12} {int(values[1]):>12,} {int(values[2]):>12,} {int(values[3]):>15,} {float(values[4]):>15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary & Deliverables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DELIVERABLES:\n",
    "\n",
    "1. CloudFormation Template:\n",
    "   s3://au2025-csed516-aravi7/capstone/capstone-starter.cfn.yaml\n",
    "\n",
    "2. Orchestration Notebook:\n",
    "   s3://au2025-csed516-aravi7/capstone/build_pipeline.ipynb (this file)\n",
    "\n",
    "3. S3 URI of Analytical Dataset:\n",
    "   s3://capstone-events-processed-aravi7-410367694421/events/\n",
    "\n",
    "4. Queries File:\n",
    "   s3://au2025-csed516-aravi7/capstone/queries.sql\n",
    "\n",
    "\n",
    "RESOURCES CREATED:\n",
    "\n",
    " - Source Bucket:    s3://capstone-events-aravi7-410367694421/\n",
    "\n",
    "- Processed Bucket: s3://capstone-events-processed-aravi7-410367694421/\n",
    "\n",
    "- Glue Database:    capstone_aravi7_db\n",
    "\n",
    "- Raw Table:        capstone_aravi7_db.raw_events\n",
    "\n",
    "- Processed Table:  capstone_aravi7_db.processed_events\n",
    "\n",
    "- ETL Job:          capstone-etl-aravi7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-science)",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

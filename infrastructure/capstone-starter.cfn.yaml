AWSTemplateFormatVersion: '2010-09-09'
Description: 'Capstone Project - Event Analytics Pipeline (Extended)'

Parameters:
  StudentId:
    Type: String
    Description: Student identifier (e.g., aravi7)
    AllowedPattern: '^[a-z0-9-]+$'
    ConstraintDescription: Must contain only lowercase letters, numbers, and hyphens

Resources:
  # Source bucket where Lambda writes raw events
  SourceEventsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'capstone-events-${StudentId}-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldEvents
            Status: Enabled
            ExpirationInDays: 30
            NoncurrentVersionExpirationInDays: 7

  # Lambda function for event generation
  EventGeneratorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'capstone-event-generator-${StudentId}'
      Runtime: python3.11
      Handler: index.lambda_handler
      Role: !Sub 'arn:aws:iam::${AWS::AccountId}:role/LabRole'
      Timeout: 900
      MemorySize: 3008
      Environment:
        Variables:
          SOURCE_BUCKET: !Ref SourceEventsBucket
      Code:
        ZipFile: |
          import json
          import gzip
          import os
          from datetime import datetime, timezone
          import random
          import boto3

          s3_client = boto3.client('s3')

          # Product catalog
          CATEGORIES = ["electronics", "clothing", "home", "books", "sports", "toys"]
          PRODUCTS = {
              "electronics": ["p_1001", "p_1002", "p_1003", "p_1004", "p_1005"],
              "clothing": ["p_2001", "p_2002", "p_2003", "p_2004", "p_2005"],
              "home": ["p_3001", "p_3002", "p_3003", "p_3004", "p_3005"],
              "books": ["p_4001", "p_4002", "p_4003", "p_4004", "p_4005"],
              "sports": ["p_5001", "p_5002", "p_5003", "p_5004", "p_5005"],
              "toys": ["p_6001", "p_6002", "p_6003", "p_6004", "p_6005"],
          }

          SEARCH_QUERIES = [
              "wireless headphones", "running shoes", "coffee maker",
              "python books", "yoga mat", "laptop stand",
              "winter jacket", "desk lamp",
          ]

          def get_random_event_count():
              return random.randint(500_000, 750_000)

          def generate_product_context():
              category = random.choice(CATEGORIES)
              product_id = random.choice(PRODUCTS[category])
              quantity = random.randint(1, 5)
              price = round(random.uniform(9.99, 299.99), 2)
              return {
                  "product_id": product_id,
                  "category": category,
                  "quantity": quantity,
                  "price": price,
              }

          def generate_event():
              timestamp = datetime.now(timezone.utc).isoformat()
              user_id = f"u_{random.randint(10000, 99999)}"
              session_id = f"s_{random.randint(10000, 99999)}"
              event_type = random.choices(
                  ["page_view", "add_to_cart", "remove_from_cart", "purchase", "search"],
                  weights=[50, 20, 10, 10, 10],
                  k=1
              )[0]

              event = {
                  "timestamp": timestamp,
                  "user_id": user_id,
                  "session_id": session_id,
                  "event_type": event_type,
                  "product_id": None,
                  "quantity": None,
                  "price": None,
                  "category": None,
                  "search_query": None,
              }

              if event_type == "search":
                  event["search_query"] = random.choice(SEARCH_QUERIES)
              else:
                  product_context = generate_product_context()
                  event["product_id"] = product_context["product_id"]
                  event["category"] = product_context["category"]
                  if event_type in ["add_to_cart", "remove_from_cart", "purchase"]:
                      event["quantity"] = product_context["quantity"]
                      event["price"] = product_context["price"]

              return event

          def generate_events_jsonl(count):
              """Generate events as JSONL string without building full list in memory."""
              lines = []
              for _ in range(count):
                  event = generate_event()
                  lines.append(json.dumps(event))
              return '\n'.join(lines)

          def lambda_handler(event, context):
              bucket_name = os.environ['SOURCE_BUCKET']
              event_count = get_random_event_count()
              print(f"Generating {event_count:,} events")

              # Generate events directly as JSONL string
              jsonl_content = generate_events_jsonl(event_count)
              print(f"Generated {len(jsonl_content):,} bytes of JSON")

              # Compress
              compressed_content = gzip.compress(jsonl_content.encode('utf-8'))
              print(f"Compressed to {len(compressed_content):,} bytes")

              now = datetime.now(timezone.utc)
              timestamp = now.strftime("%Y%m%d-%H%M%S")
              s3_key = f"events/year={now.year:04d}/month={now.month:02d}/day={now.day:02d}/hour={now.hour:02d}/minute={now.minute:02d}/events-{timestamp}.jsonl.gz"

              s3_client.put_object(
                  Bucket=bucket_name,
                  Key=s3_key,
                  Body=compressed_content,
                  ContentType='application/gzip',
                  ContentEncoding='gzip'
              )

              print(f"Uploaded {event_count:,} events to s3://{bucket_name}/{s3_key}")

              return {
                  "statusCode": 200,
                  "body": json.dumps({
                      "event_count": event_count,
                      "s3_key": s3_key,
                      "bucket": bucket_name
                  })
              }

  # EventBridge rule to trigger Lambda every 5 minutes
  EventGeneratorSchedule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub 'capstone-event-generator-schedule-${StudentId}'
      Description: Trigger event generator every 5 minutes
      ScheduleExpression: 'rate(5 minutes)'
      State: ENABLED
      Targets:
        - Arn: !GetAtt EventGeneratorFunction.Arn
          Id: EventGeneratorTarget

  # Allow EventBridge to invoke Lambda
  EventGeneratorFunctionPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref EventGeneratorFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EventGeneratorSchedule.Arn

  # Lambda function to empty buckets on stack deletion
  BucketCleanupFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'capstone-bucket-cleanup-${StudentId}'
      Runtime: python3.11
      Handler: index.handler
      Role: !Sub 'arn:aws:iam::${AWS::AccountId}:role/LabRole'
      Timeout: 300
      Code:
        ZipFile: |
          import boto3
          import cfnresponse

          def handler(event, context):
              try:
                  if event['RequestType'] == 'Delete':
                      bucket_name = event['ResourceProperties']['BucketName']
                      print(f"Emptying bucket: {bucket_name}")
                      s3 = boto3.resource('s3')
                      bucket = s3.Bucket(bucket_name)
                      # Delete all objects including versions
                      bucket.object_versions.all().delete()
                      print(f"Bucket {bucket_name} emptied successfully")
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                  print(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})

  # Custom resource to trigger source bucket cleanup on stack deletion
  SourceBucketCleanup:
    Type: Custom::BucketCleanup
    Properties:
      ServiceToken: !GetAtt BucketCleanupFunction.Arn
      BucketName: !Ref SourceEventsBucket

  # =============================================================================
  # PIPELINE RESOURCES (New additions for ETL pipeline)
  # =============================================================================

  # Processed events bucket (Parquet output)
  ProcessedEventsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'capstone-events-processed-${StudentId}-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldProcessedEvents
            Status: Enabled
            ExpirationInDays: 30
            NoncurrentVersionExpirationInDays: 7

  # Custom resource to trigger processed bucket cleanup on stack deletion
  ProcessedBucketCleanup:
    Type: Custom::BucketCleanup
    DependsOn: ProcessedEventsBucket
    Properties:
      ServiceToken: !GetAtt BucketCleanupFunction.Arn
      BucketName: !Ref ProcessedEventsBucket

  # Glue Database for the analytics pipeline
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub 'capstone_${StudentId}_db'
        Description: 'Capstone project database for event analytics'

  # Glue Table for raw events (JSON source)
  GlueRawEventsTable:
    Type: AWS::Glue::Table
    DependsOn: GlueDatabase
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Sub 'capstone_${StudentId}_db'
      TableInput:
        Name: 'raw_events'
        Description: 'Raw e-commerce events in JSON format'
        TableType: 'EXTERNAL_TABLE'
        Parameters:
          classification: 'json'
          compressionType: 'gzip'
          typeOfData: 'file'
        StorageDescriptor:
          Location: !Sub 's3://${SourceEventsBucket}/events/'
          InputFormat: 'org.apache.hadoop.mapred.TextInputFormat'
          OutputFormat: 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
          SerdeInfo:
            SerializationLibrary: 'org.openx.data.jsonserde.JsonSerDe'
            Parameters:
              paths: 'timestamp,user_id,session_id,event_type,product_id,quantity,price,category,search_query'
          Columns:
            - Name: timestamp
              Type: string
              Comment: 'Event timestamp in ISO 8601 format'
            - Name: user_id
              Type: string
              Comment: 'User identifier'
            - Name: session_id
              Type: string
              Comment: 'Session identifier'
            - Name: event_type
              Type: string
              Comment: 'Type of event: page_view, add_to_cart, remove_from_cart, purchase, search'
            - Name: product_id
              Type: string
              Comment: 'Product identifier (null for search events)'
            - Name: quantity
              Type: int
              Comment: 'Quantity for cart/purchase events'
            - Name: price
              Type: double
              Comment: 'Price for cart/purchase events'
            - Name: category
              Type: string
              Comment: 'Product category (null for search events)'
            - Name: search_query
              Type: string
              Comment: 'Search query (only for search events)'
        PartitionKeys:
          - Name: year
            Type: string
          - Name: month
            Type: string
          - Name: day
            Type: string
          - Name: hour
            Type: string
          - Name: minute
            Type: string

  # Glue Table for processed events (Parquet output)
  GlueProcessedEventsTable:
    Type: AWS::Glue::Table
    DependsOn: GlueDatabase
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Sub 'capstone_${StudentId}_db'
      TableInput:
        Name: 'processed_events'
        Description: 'Processed e-commerce events in Parquet format'
        TableType: 'EXTERNAL_TABLE'
        Parameters:
          classification: 'parquet'
          typeOfData: 'file'
        StorageDescriptor:
          Location: !Sub 's3://${ProcessedEventsBucket}/events/'
          InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
          OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
          SerdeInfo:
            SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
          Columns:
            - Name: timestamp
              Type: timestamp
              Comment: 'Event timestamp'
            - Name: user_id
              Type: string
              Comment: 'User identifier'
            - Name: session_id
              Type: string
              Comment: 'Session identifier'
            - Name: event_type
              Type: string
              Comment: 'Type of event: page_view, add_to_cart, remove_from_cart, purchase, search'
            - Name: product_id
              Type: string
              Comment: 'Product identifier (null for search events)'
            - Name: quantity
              Type: int
              Comment: 'Quantity for cart/purchase events'
            - Name: price
              Type: double
              Comment: 'Price for cart/purchase events'
            - Name: category
              Type: string
              Comment: 'Product category (null for search events)'
            - Name: search_query
              Type: string
              Comment: 'Search query (only for search events)'
            - Name: event_date
              Type: date
              Comment: 'Derived date from timestamp'
            - Name: event_hour
              Type: int
              Comment: 'Derived hour from timestamp (0-23)'
        PartitionKeys:
          - Name: year
            Type: string
          - Name: month
            Type: string
          - Name: day
            Type: string
          - Name: hour
            Type: string

  # Glue ETL Job for transforming raw JSON to Parquet
  GlueETLJob:
    Type: AWS::Glue::Job
    DependsOn:
      - GlueDatabase
      - GlueRawEventsTable
      - GlueProcessedEventsTable
    Properties:
      Name: !Sub 'capstone-etl-${StudentId}'
      Description: 'ETL job to transform raw JSON events to Parquet format'
      Role: !Sub 'arn:aws:iam::${AWS::AccountId}:role/LabRole'
      GlueVersion: '4.0'
      WorkerType: 'G.1X'
      NumberOfWorkers: 2
      Timeout: 60
      Command:
        Name: 'glueetl'
        ScriptLocation: !Sub 's3://${ProcessedEventsBucket}/scripts/etl_job.py'
        PythonVersion: '3'
      DefaultArguments:
        '--job-bookmark-option': 'job-bookmark-enable'
        '--TempDir': !Sub 's3://${ProcessedEventsBucket}/temp/'
        '--enable-metrics': 'true'
        '--enable-spark-ui': 'true'
        '--spark-event-logs-path': !Sub 's3://${ProcessedEventsBucket}/spark-logs/'
        '--enable-continuous-cloudwatch-log': 'true'
        '--SOURCE_DATABASE': !Sub 'capstone_${StudentId}_db'
        '--SOURCE_TABLE': 'raw_events'
        '--TARGET_PATH': !Sub 's3://${ProcessedEventsBucket}/events/'
        '--TARGET_DATABASE': !Sub 'capstone_${StudentId}_db'
        '--TARGET_TABLE': 'processed_events'

Outputs:
  # Original outputs
  SourceBucketName:
    Description: S3 bucket containing raw event data
    Value: !Ref SourceEventsBucket
    Export:
      Name: !Sub '${AWS::StackName}-SourceBucket'

  SourceDataPrefix:
    Description: S3 prefix where events are written
    Value: 'events/'
    Export:
      Name: !Sub '${AWS::StackName}-SourcePrefix'

  SourceBucketArn:
    Description: ARN of source bucket
    Value: !GetAtt SourceEventsBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-SourceBucketArn'

  EventGeneratorSchedule:
    Description: EventBridge rule triggering event generation
    Value: !Ref EventGeneratorSchedule
    Export:
      Name: !Sub '${AWS::StackName}-Schedule'

  LambdaFunctionName:
    Description: Name of event generator Lambda function
    Value: !Ref EventGeneratorFunction
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunction'

  # New pipeline outputs
  ProcessedBucketName:
    Description: S3 bucket containing processed Parquet data
    Value: !Ref ProcessedEventsBucket
    Export:
      Name: !Sub '${AWS::StackName}-ProcessedBucket'

  ProcessedDataLocation:
    Description: S3 URI of the analytical dataset (processed events)
    Value: !Sub 's3://${ProcessedEventsBucket}/events/'
    Export:
      Name: !Sub '${AWS::StackName}-ProcessedDataLocation'

  GlueDatabaseName:
    Description: Glue database name
    Value: !Sub 'capstone_${StudentId}_db'
    Export:
      Name: !Sub '${AWS::StackName}-GlueDatabase'

  GlueRawTableName:
    Description: Glue table for raw events
    Value: 'raw_events'
    Export:
      Name: !Sub '${AWS::StackName}-RawTable'

  GlueProcessedTableName:
    Description: Glue table for processed events
    Value: 'processed_events'
    Export:
      Name: !Sub '${AWS::StackName}-ProcessedTable'

  GlueETLJobName:
    Description: Glue ETL job name
    Value: !Sub 'capstone-etl-${StudentId}'
    Export:
      Name: !Sub '${AWS::StackName}-ETLJob'
